"""
RAG Engine –¥–ª—è –ø–æ—à—É–∫—É –≤ –∞—Ä—Ö—ñ–≤–∞—Ö
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Sentence-BERT + –µ–≤—Ä–∏—Å—Ç–∏—á–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
from typing import List, Dict
from fuzzywuzzy import fuzz
import re


class RAGEngine:
    def __init__(self, archives_path: str = "data/archives.json"):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG engine"""
        print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Sentence-BERT –º–æ–¥–µ–ª—ñ...")
        # Multilingual –º–æ–¥–µ–ª—å –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—Ä—Ö—ñ–≤—ñ–≤
        with open(archives_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.archives = data['archives']
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –∞—Ä—Ö—ñ–≤—ñ–≤
        print("üîÑ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤...")
        self.archive_texts = [
            f"{arch['title']} {arch['content']} {' '.join(arch['metadata'].get('surnames', []))}"
            for arch in self.archives
        ]
        self.archive_embeddings = self.model.encode(self.archive_texts)
        print(f"‚úÖ RAG –≥–æ—Ç–æ–≤–∏–π! –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.archives)} –∞—Ä—Ö—ñ–≤–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
    
    def search(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict]:
        """
        –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø–æ—à—É–∫—É
        
        Args:
            query: –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–Ω–∞–ø—Ä. "–º—ñ–π –ø—Ä–∞–¥—ñ–¥ –ª—ñ–∫–∞—Ä –ö–∏—ó–≤ 1920-—Ö")
            top_k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            threshold: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ —Å—Ö–æ–∂–æ—Å—Ç—ñ (0-1)
        
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –∑ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º
        """
        print(f"\nüîç –ü–æ—à—É–∫: '{query}'")
        
        # 1. –í–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫ (Sentence-BERT)
        query_embedding = self.model.encode([query])
        similarities = cosine_similarity(query_embedding, self.archive_embeddings)[0]
        
        # –¢–æ–ø-K –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
        top_indices = np.argsort(similarities)[::-1][:top_k * 2]  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
        
        # 2. –î–æ–¥–∞—î–º–æ –µ–≤—Ä–∏—Å—Ç–∏—á–Ω–∏–π scoring
        results = []
        for idx in top_indices:
            if similarities[idx] < threshold:
                continue
            
            archive = self.archives[idx]
            
            # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π score
            semantic_score = float(similarities[idx])
            heuristic_score, explanation = self._calculate_heuristic_score(query, archive)
            
            final_score = (semantic_score * 0.6) + (heuristic_score * 0.4)
            
            results.append({
                "id": archive['id'],
                "title": archive['title'],
                "content": archive['content'],
                "year": archive['year'],
                "location": archive['location'],
                "semantic_score": round(semantic_score, 3),
                "heuristic_score": round(heuristic_score, 3),
                "confidence_score": round(final_score, 3),
                "explanation": explanation,
                "metadata": archive['metadata']
            })
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º score
        results.sort(key=lambda x: x['confidence_score'], reverse=True)
        
        return results[:top_k]
    
    def _calculate_heuristic_score(self, query: str, archive: Dict) -> tuple:
        """
        –ï–≤—Ä–∏—Å—Ç–∏—á–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ
        
        Returns:
            (score, explanation): Score 0-1 —ñ —Ç–µ–∫—Å—Ç–æ–≤–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è
        """
        query_lower = query.lower()
        score = 0.0
        explanations = []
        
        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä—ñ–∑–≤–∏—â (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ!)
        surnames_in_query = self._extract_surnames(query)
        archive_surnames = archive['metadata'].get('surnames', [])
        
        for query_surname in surnames_in_query:
            for archive_surname in archive_surnames:
                fuzzy_score = fuzz.ratio(query_surname.lower(), archive_surname.lower())
                if fuzzy_score > 80:  # –°—Ö–æ–∂—ñ—Å—Ç—å >80%
                    score += 0.4
                    explanations.append(
                        f"–ü—Ä—ñ–∑–≤–∏—â–µ '{query_surname}' –∑–±—ñ–≥–∞—î—Ç—å—Å—è –∑ '{archive_surname}' "
                        f"(—Å—Ö–æ–∂—ñ—Å—Ç—å {fuzzy_score}%)"
                    )
                    break
        
        # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∫—ñ–≤
        years_in_query = self._extract_years(query)
        if years_in_query:
            archive_year = archive['year']
            for query_year in years_in_query:
                year_diff = abs(archive_year - query_year)
                if year_diff <= 5:
                    year_score = 0.3 * (1 - year_diff / 10)  # –ß–∏–º –±–ª–∏–∂—á–µ, —Ç–∏–º –∫—Ä–∞—â–µ
                    score += year_score
                    explanations.append(
                        f"–†—ñ–∫ {archive_year} –±–ª–∏–∑—å–∫–∏–π –¥–æ –∑–∞–ø–∏—Ç—É {query_year} (¬±{year_diff} —Ä–æ–∫—ñ–≤)"
                    )
        
        # 3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π (–∑ —Å–∏–Ω–æ–Ω—ñ–º–∞–º–∏)
        occupation_keywords = {
            '–ª—ñ–∫–∞—Ä': ['–ª—ñ–∫–∞—Ä', '–¥–æ–∫—Ç–æ—Ä', '–º–µ–¥–∏–∫', '—Ç–µ—Ä–∞–ø–µ–≤—Ç'],
            '–≤—á–∏—Ç–µ–ª—å': ['–≤—á–∏—Ç–µ–ª—å', '—É—á–∏—Ç–µ–ª—å', '–ø–µ–¥–∞–≥–æ–≥', '–≤–∏–∫–ª–∞–¥–∞—á'],
            '—Å–µ–ª—è–Ω–∏–Ω': ['—Å–µ–ª—è–Ω–∏–Ω', '–∑–µ–º–ª–µ—Ä–æ–±', '—Ö–ª—ñ–±–æ—Ä–æ–±'],
            '–¥—è–∫': ['–¥—è–∫', '—Ü–µ—Ä–∫–æ–≤–Ω–æ—Å–ª—É–∂–∏—Ç–µ–ª—å']
        }
        
        for main_occupation, synonyms in occupation_keywords.items():
            if any(syn in query_lower for syn in synonyms):
                # –®—É–∫–∞—î–º–æ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ —Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
                archive_text = (archive['content'] + ' ' + 
                               archive['metadata'].get('father_occupation', '') + ' ' +
                               archive['metadata'].get('mother_family', '')).lower()
                
                if any(syn in archive_text for syn in synonyms):
                    score += 0.2
                    explanations.append(
                        f"–ü—Ä–æ—Ñ–µ—Å—ñ—è '{main_occupation}' –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ"
                    )
                    break
        
        # 4. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ª–æ–∫–∞—Ü—ñ–π (–∑ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è)
        location_variants = archive['metadata'].get('location_variants', [])
        for location in location_variants:
            if location.lower() in query_lower:
                score += 0.1
                explanations.append(f"–õ–æ–∫–∞—Ü—ñ—è '{location}' –∑–±—ñ–≥–∞—î—Ç—å—Å—è")
                break
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ score –¥–æ 0-1
        score = min(score, 1.0)
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î –ø–æ—è—Å–Ω–µ–Ω—å, –¥–æ–¥–∞—î–º–æ –∑–∞–≥–∞–ª—å–Ω–µ
        if not explanations:
            explanations.append("–ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É")
        
        return score, " ‚Ä¢ ".join(explanations)
    
    def _extract_surnames(self, text: str) -> List[str]:
        """–í–∏—Ç—è–≥—É—î –ø—Ä—ñ–∑–≤–∏—â–∞ –∑ —Ç–µ–∫—Å—Ç—É (–µ–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å–ª–æ–≤–∞ –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏)"""
        # –ü—Ä–æ—Å—Ç—ñ –ø—Ä–∞–≤–∏–ª–∞: —Å–ª–æ–≤–∞ –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏, –¥–æ–≤—à—ñ –∑–∞ 3 —Å–∏–º–≤–æ–ª–∏
        words = text.split()
        surnames = []
        
        ukrainian_surnames_patterns = [
            r'\b[–ê-–Ø“ê–Ñ–Ü–á][–∞-—è“ë—î—ñ—ó]{2,}(?:–µ–Ω–∫–æ|enko|—É–∫|—é–∫|—Å—å–∫–∏–π|—Ü—å–∫–∏–π|–∏—á|–æ–≤–∏—á|—î–≤–∏—á)\b',
            r'\b[–ê-–Ø“ê–Ñ–Ü–á][–∞-—è“ë—î—ñ—ó]{3,}\b'
        ]
        
        for pattern in ukrainian_surnames_patterns:
            surnames.extend(re.findall(pattern, text))
        
        return list(set(surnames))  # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ
    
    def _extract_years(self, text: str) -> List[int]:
        """–í–∏—Ç—è–≥—É—î —Ä–æ–∫–∏ –∑ —Ç–µ–∫—Å—Ç—É"""
        # –®—É–∫–∞—î–º–æ 4-–∑–Ω–∞—á–Ω—ñ —á–∏—Å–ª–∞, —Å—Ö–æ–∂—ñ –Ω–∞ —Ä–æ–∫–∏
        years = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–æ–∫—ñ–≤
        year_patterns = [
            r'\b(19\d{2}|20\d{2})\b',  # –¢–æ—á–Ω—ñ —Ä–æ–∫–∏ (1900-2099)
            r'\b(\d{2})-—Ö\b'  # –î–µ—Å—è—Ç–∏–ª—ñ—Ç—Ç—è (20-—Ö, 30-—Ö)
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if len(match) == 4:
                    years.append(int(match))
                elif len(match) == 2:  # –î–µ—Å—è—Ç–∏–ª—ñ—Ç—Ç—è
                    # 20-—Ö -> 1920
                    decade = int(match)
                    if decade < 30:
                        years.append(2000 + decade * 10)
                    else:
                        years.append(1900 + decade * 10)
        
        return years


# ============ –¢–ï–°–¢–ò ============
if __name__ == "__main__":
    print("=" * 60)
    print("üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø RAG ENGINE")
    print("=" * 60)
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    rag = RAGEngine()
    
    # –¢–µ—Å—Ç 1: –ü–æ—à—É–∫ –ø–æ –ø—Ä—ñ–∑–≤–∏—â—É + –ø—Ä–æ—Ñ–µ—Å—ñ—è + –º—ñ—Å—Ç–æ
    print("\n\nüìù –¢–µ—Å—Ç 1: '–º—ñ–π –ø—Ä–∞–¥—ñ–¥ –ª—ñ–∫–∞—Ä –ö–æ–≤–∞–ª–µ–Ω–∫–æ –ö–∏—ó–≤ 1920-—Ö'")
    results = rag.search("–º—ñ–π –ø—Ä–∞–¥—ñ–¥ –ª—ñ–∫–∞—Ä –ö–æ–≤–∞–ª–µ–Ω–∫–æ –ö–∏—ó–≤ 1920-—Ö", top_k=3)
    
    for i, result in enumerate(results, 1):
        print(f"\n{'='*60}")
        print(f"üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç #{i} (Score: {result['confidence_score']})")
        print(f"üìÑ {result['title']}")
        print(f"üìÖ {result['year']} | üìç {result['location']}")
        print(f"\nüí° –ü–æ—è—Å–Ω–µ–Ω–Ω—è:")
        print(f"   {result['explanation']}")
        print(f"\nüìñ –£—Ä–∏–≤–æ–∫:")
        print(f"   {result['content'][:200]}...")
    
    # –¢–µ—Å—Ç 2: –ü–æ—à—É–∫ –ø–æ –≤–∞—Ä—ñ–∞–Ω—Ç–∞—Ö –ø—Ä—ñ–∑–≤–∏—â–∞
    print("\n\nüìù –¢–µ—Å—Ç 2: '–ö–æ–≤–∞–ª—î–Ω–∫–æ —Å–µ–ª–æ –ü–∏—Ä–æ–≥—ñ–≤ 1890'")
    results = rag.search("–ö–æ–≤–∞–ª—î–Ω–∫–æ —Å–µ–ª–æ –ü–∏—Ä–æ–≥—ñ–≤ 1890", top_k=2)
    
    for i, result in enumerate(results, 1):
        print(f"\n{'='*60}")
        print(f"üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç #{i} (Score: {result['confidence_score']})")
        print(f"üìÑ {result['title']}")
        print(f"\nüí° {result['explanation']}")
    
    print("\n\n‚úÖ –¢–µ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ñ!")